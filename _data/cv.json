{
  "basics": {
    "name": "Sergei Makeev",
    "email": "neuralsrg@gmail.com",
    "phone": "",
    "website": "https://neuralsrg.github.io",
    "summary": "MSc at Moscow State University (expected), RecSys R&D at Yandex",
    "location": {
      "address": "",
      "postalCode": "",
      "city": "Moscow",
      "countryCode": "RU",
      "region": ""
    },
    "profiles": [
      {
        "network": "Google Scholar",
        "username": "",
        "url": "https://scholar.google.com/citations?user=rOJPT8oAAAAJ&hl=en&oi=sra"
      },
      {
        "network": "ORCID",
        "username": "",
        "url": "https://orcid.org/0009-0003-5451-6475"
      },
      {
        "network": "GitHub",
        "username": "neuralsrg",
        "url": "https://github.com/neuralsrg"
      }
    ]
  },
  "work": [],
  "education": [
    {
      "institution": "Moscow State University",
      "area": "M.S. in Applied Mathematics and Computer Science",
      "studyType": "",
      "startDate": "2024",
      "endDate": "",
      "gpa": null,
      "courses": []
    },
    {
      "institution": "Moscow State University",
      "area": "B.S. in Applied Mathematics and Computer Science",
      "studyType": "",
      "startDate": "2020",
      "endDate": "2024",
      "gpa": null,
      "courses": []
    }
  ],
  "skills": [],
  "languages": [],
  "interests": [],
  "references": [],
  "publications": [
    {
      "name": "Scaling Recommender Transformers to One Billion Parameters",
      "publisher": "ArXiv",
      "releaseDate": "2025-07-21",
      "website": "https://www.arxiv.org/abs/2507.15994",
      "summary": "We proposed a two-stage training approach for a ranking causal transformer. In the first stage, we pretrain the model on a fundamental next-token prediction task, which is essential for enabling scale-up of the encoder. In the second stage, we fine-tune the model on a pairwise ranking objective. We evaluate performance across four model sizes with exponentially increasing encoder capacity and demonstrate adherence to the scaling law."
    },
    {
      "name": "Correcting the LogQ Correction: Revisiting Sampled Softmax for Large-Scale Retrieval",
      "publisher": "Proceedings of the 19th ACM Conference on Recommender Systems (RecSys’25), 2025",
      "releaseDate": "2025-07-12",
      "website": "https://arxiv.org/abs/2507.09331",
      "summary": "We revisited the derivation of the LogQ correction and identified an inconsistency related to the treatment of the positive sample in the denominator. By explicitly accounting for the positive sample during Monte Carlo sampling, we derived a new formulation that introduces an intuitive reweighting coefficient. Interestingly, the modified loss function achieves superior performance on both open datasets and large-scale proprietary benchmarks."
    },
    {
      "name": "Blending Sequential Embeddings, Graphs, and Engineered Features: 4th Place Solution in RecSys Challenge 2025",
      "publisher": "Proceedings of the 19th ACM Conference on Recommender Systems (RecSys’25), 2025",
      "releaseDate": "2025-08-09",
      "website": "https://arxiv.org/abs/2508.06970",
      "summary": "I am excited to share our work from the RecSys Challenge 2025! In our solution, we combined the Transformer, TwHIN graph neural network, DCN-v2, and hundreds of carefully engineered numeric features. We discuss feature engineering in detail in the Appendix, which one of our reviewers called a “goldmine of practical insights”. I presented our paper at RecSys 2025 in Prague."
    }
  ],
  "presentations": [
    {
      "name": "4th Place Solution in RecSys Challenge 2025",
      "event": "RecSys'25",
      "date": "2012-09-22",
      "location": "Prague, Czech Republic",
      "description": ""
    }
  ]
}